{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "# import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "# import tensorflow_text\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from random import sample \n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Experimental Parameters'''\n",
    "\n",
    "\n",
    "LSTM_Units=100\n",
    "Embedding_layer_size=20\n",
    "sequence_length=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data(path):\n",
    "    '''read sequences from '''\n",
    "    with open(path, 'r') as f:\n",
    "        seqs= f.read().split('\\n')\n",
    "    return seqs\n",
    "\n",
    "def filter_sequence(seqs, min_length=3):\n",
    "    '''filter a sequnce on minimum length'''\n",
    "    filtered_seqs=[]\n",
    "        \n",
    "    for val in seqs:\n",
    "        temp= [value for value in val.split(\" \") if value != '']\n",
    "        if len(temp)>= min_length:\n",
    "            filtered_seqs.append(val)\n",
    "    return filtered_seqs\n",
    "\n",
    "\n",
    "def train_test_split(data, train_ratio=0.25):\n",
    "    '''shuffle and split data into train test split'''\n",
    "    random.shuffle(data)\n",
    "    train_range= int(len(data)*train_ratio)\n",
    "    return data[:train_range], data[train_range:]\n",
    "\n",
    "def merge_seq_arr(arr):\n",
    "    '''Merge array of items to space seperated sequence'''\n",
    "    seq=\"\"\n",
    "    for item in arr: \n",
    "         seq+=item + \" \"\n",
    "    return seq\n",
    "    \n",
    "def split_fixed_length_sequence(data, max_length=9):\n",
    "    '''split sequences on fixed lengths to increse data points'''\n",
    "    for seq in data: \n",
    "        if len(seq)> max_length:\n",
    "            pass\n",
    "\n",
    "        \n",
    "def chunks(lst, n=8):\n",
    "    \"\"\"return successive n-sized chunks from lst.\"\"\"\n",
    "    slices=[]\n",
    "    lst = lst.split(\" \")\n",
    "    for i in range(n, len(lst)+n, n):\n",
    "        slices.append(merge_seq_arr(lst[i-n:i]))\n",
    "\n",
    "    return slices\n",
    "\n",
    "def extend_data_points(data, chunk_size=9):\n",
    "    extended_data=[]\n",
    "    for seqs in data:\n",
    "        extended_data+=chunks(seqs, chunk_size)\n",
    "    return extended_data\n",
    "\n",
    "\n",
    "\n",
    "def create_vocab(base_seq):\n",
    "    token_mapping={\"<pad>\":1.0}\n",
    "    iter_val=2.0\n",
    "\n",
    "    for seqs in base_seq:\n",
    "        tokens=[value for value in seqs.split(\" \") if value != '']\n",
    "        unique_token=list(set(tokens))\n",
    "\n",
    "        for token in unique_token:\n",
    "            if token not in token_mapping:\n",
    "                token_mapping[token] = iter_val\n",
    "                iter_val+=1\n",
    "\n",
    "    print(iter_val)\n",
    "\n",
    "    import json \n",
    "\n",
    "    with open(\"vocab_.json\", \"w\") as outfile:\n",
    "        json.dump(token_mapping, outfile)\n",
    "        \n",
    "    return token_mapping \n",
    "\n",
    "\n",
    "def tokenizer(token_mapping, seqs, max_length=3 ):\n",
    "    \n",
    "    labels=[]\n",
    "    input_ids=[]\n",
    "    \n",
    "    for seq in tqdm(seqs):\n",
    "        encoded_sequence=[]\n",
    "        mask_sequence=[]\n",
    "        \n",
    "\n",
    "        \n",
    "        #breaking sequence in tokens\n",
    "        seq=[value for value in seq.split(\" \") if value != '']\n",
    "        \n",
    "        #appending encoded items to sequence\n",
    "        for token in seq:\n",
    "            encoded_sequence.append(token_mapping[token])\n",
    "            \n",
    "        \n",
    "\n",
    "        labels.append(encoded_sequence.pop())\n",
    "        \n",
    "        \n",
    "        #padding to max length\n",
    "        if len(encoded_sequence)< max_length:\n",
    "            for i in range(max_length-len(encoded_sequence)):\n",
    "                encoded_sequence.append(token_mapping['<pad>'])\n",
    "        \n",
    "\n",
    "        input_ids.append(np.array(encoded_sequence))\n",
    "\n",
    "    return labels, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize model input data lists\n",
    "input_ids =[]\n",
    "mask =[]\n",
    "labels= []\n",
    "path = \"seqs.txt\"\n",
    "\n",
    "    \n",
    "base_seq= read_data(path)\n",
    "\n",
    "token_mapping= create_vocab(base_seq)\n",
    "\n",
    "fil_seq= filter_sequence(base_seq, min_length=sequence_length-1)\n",
    "test_seq, train_seq= train_test_split(fil_seq)\n",
    "\n",
    "updated_train= extend_data_points(train_seq, chunk_size=sequence_length+1)\n",
    "updated_test= extend_data_points(test_seq, chunk_size=sequence_length+1)\n",
    "\n",
    "updated_train= filter_sequence(updated_train)\n",
    "updated_test= filter_sequence(updated_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75680/75680 [00:00<00:00, 564963.83it/s]\n",
      "100%|██████████| 22412/22412 [00:00<00:00, 571945.91it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "labels_train, input_ids_train= tokenizer(token_mapping, updated_train, max_length=sequence_length)\n",
    "labels_test, input_ids_test= tokenizer(token_mapping, updated_test, max_length=sequence_length)\n",
    "\n",
    "vocab_size= len(token_mapping.keys()) +1\n",
    "\n",
    "\n",
    "X_train, Y_train=np.array(input_ids_train), np.array(labels_train)\n",
    "X_test, Y_test=np.array(input_ids_test), np.array(labels_test)\n",
    "\n",
    "\n",
    "n_values = len(token_mapping) +1\n",
    "labels_train= np.eye(n_values)[Y_train.astype(int)]\n",
    "labels_test= np.eye(n_values)[Y_test.astype(int)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_steps_in, n_steps_out = 3, 1\n",
    "# # covert into input/output\n",
    "# X = np.load('data/X.data.npy' , allow_pickle=True)\n",
    "# y = np.load('data/y.data.npy',allow_pickle=True)\n",
    "# print(X.shape)\n",
    "# # the dataset knows the number of features, e.g. 2\n",
    "# n_features = 1\n",
    "\n",
    "# embedding_vecor_length = 64\n",
    "# vocab_size=len(y[0])\n",
    "# # max_length=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2365/2365 [==============================] - 24s 10ms/step - loss: 5.2877 - accuracy: 0.0203 - val_loss: 4.9153 - val_accuracy: 0.0340\n",
      "Epoch 2/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.8298 - accuracy: 0.0322 - val_loss: 4.8224 - val_accuracy: 0.0371\n",
      "Epoch 3/100\n",
      "2365/2365 [==============================] - 24s 10ms/step - loss: 4.7317 - accuracy: 0.0368 - val_loss: 4.7784 - val_accuracy: 0.0403\n",
      "Epoch 4/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.6649 - accuracy: 0.0417 - val_loss: 4.7432 - val_accuracy: 0.0453\n",
      "Epoch 5/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.6119 - accuracy: 0.0468 - val_loss: 4.7186 - val_accuracy: 0.0481\n",
      "Epoch 6/100\n",
      "2365/2365 [==============================] - 23s 10ms/step - loss: 4.5676 - accuracy: 0.0514 - val_loss: 4.7162 - val_accuracy: 0.0498\n",
      "Epoch 7/100\n",
      "2365/2365 [==============================] - 23s 10ms/step - loss: 4.5261 - accuracy: 0.0561 - val_loss: 4.7109 - val_accuracy: 0.0515\n",
      "Epoch 8/100\n",
      "2365/2365 [==============================] - 23s 10ms/step - loss: 4.4919 - accuracy: 0.0596 - val_loss: 4.7052 - val_accuracy: 0.0531\n",
      "Epoch 9/100\n",
      "2365/2365 [==============================] - 23s 10ms/step - loss: 4.4541 - accuracy: 0.0647 - val_loss: 4.7038 - val_accuracy: 0.0532\n",
      "Epoch 10/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.4231 - accuracy: 0.0689 - val_loss: 4.7207 - val_accuracy: 0.0535\n",
      "Epoch 11/100\n",
      "2365/2365 [==============================] - 23s 10ms/step - loss: 4.3924 - accuracy: 0.0729 - val_loss: 4.7183 - val_accuracy: 0.0567\n",
      "Epoch 12/100\n",
      "2365/2365 [==============================] - 21s 9ms/step - loss: 4.3656 - accuracy: 0.0748 - val_loss: 4.7282 - val_accuracy: 0.0589\n",
      "Epoch 13/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.3357 - accuracy: 0.0798 - val_loss: 4.7446 - val_accuracy: 0.0571\n",
      "Epoch 14/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.3081 - accuracy: 0.0831 - val_loss: 4.7593 - val_accuracy: 0.0580\n",
      "Epoch 15/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.2832 - accuracy: 0.0866 - val_loss: 4.7670 - val_accuracy: 0.0596\n",
      "Epoch 16/100\n",
      "2365/2365 [==============================] - 21s 9ms/step - loss: 4.2619 - accuracy: 0.0885 - val_loss: 4.7848 - val_accuracy: 0.0612\n",
      "Epoch 17/100\n",
      "2365/2365 [==============================] - 22s 9ms/step - loss: 4.2410 - accuracy: 0.0917 - val_loss: 4.7927 - val_accuracy: 0.0614\n",
      "Epoch 18/100\n",
      "2364/2365 [============================>.] - ETA: 0s - loss: 4.2192 - accuracy: 0.0946"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, Embedding_layer_size))\n",
    "\n",
    "model.add(Bidirectional(LSTM(LSTM_Units, activation='relu', dropout=0.4, recurrent_dropout=0.2)))\n",
    "\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit model\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=25, restore_best_weights=True)\n",
    "model.fit(X_train, labels_train, batch_size=32, epochs=100, verbose=1,  validation_data=(X_test, labels_test), callbacks=[es])\n",
    "# demonstrate prediction\n",
    "\n",
    "model.save('data/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
