{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from IPython.display import clear_output\n",
    "from gensim.models import Word2Vec, tfidfmodel\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = 'upwork_proj.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "ETHEREUM_COMMON = 'blockchain-etl.ethereum_common'\n",
    "TRANSFERSINGLE = f'{ETHEREUM_COMMON}.All_event_TransferSingle'\n",
    "TRANSFERBATCH = f'{ETHEREUM_COMMON}.All_event_TransferBatch'\n",
    "PARALLEL_ALPHA_CONTRACT = f'0x76BE3b62873462d2142405439777e971754E8E77'\n",
    "\n",
    "df = client.query(f\"\"\"\n",
    "SELECT ts.to as address, ts.id as item, ts.block_timestamp as timestamp\n",
    "FROM {TRANSFERSINGLE} as ts\n",
    "WHERE LOWER(contract_address) like LOWER('{PARALLEL_ALPHA_CONTRACT}')\n",
    "UNION ALL\n",
    "SELECT tb.to as address, item, tb.block_timestamp as timestamp\n",
    "FROM {TRANSFERBATCH} as tb\n",
    "CROSS JOIN UNNEST(SPLIT(tb.ids, ',')) as item\n",
    "WHERE LOWER(contract_address) like LOWER('{PARALLEL_ALPHA_CONTRACT}')\n",
    "\"\"\").to_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65551 sequences from 65551 addresses with an average length of 11.85.\n",
      "65551 sequences, avg length 11.85, avg X 10.85.  29396 valid training samples.\n"
     ]
    }
   ],
   "source": [
    "seqs = df.sort_values('timestamp').groupby('address')['item'].apply(list).to_frame(name='items').reset_index().set_index('address')\n",
    "seqs = seqs.drop('0x0000000000000000000000000000000000000000')\n",
    "seqs['X'] = seqs['items'].apply(lambda l: [a for a in l])  # DEEP copy\n",
    "seqs['Y'] = seqs['X'].apply(lambda x: x.pop())\n",
    "# seqs['include'] = seqs['X'].apply(lambda x: len(x) > 0)\n",
    "seqs['length'] = seqs['items'].map(len)\n",
    "print(f'{len(seqs)} sequences from {len(seqs.index.unique())} addresses with an average length of {seqs[\"items\"].apply(len).mean():.2f}.')\n",
    "print(f'{len(seqs)} sequences, avg length {seqs[\"items\"].apply(len).mean():.2f}, avg X {seqs[\"X\"].apply(len).mean():.2f}.  {len(seqs[seqs[\"length\"]>1])} valid training samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def read_data(path):\n",
    "    '''read sequences from '''\n",
    "    with open(path, 'r') as f:\n",
    "        seqs= f.read().split('\\n')\n",
    "    return seqs\n",
    "\n",
    "def filter_sequence(seqs, min_length=3):\n",
    "    '''filter a sequnce on minimum length'''\n",
    "    filtered_seqs=[]\n",
    "    for val in seqs:\n",
    "        size=val.split(\" \")\n",
    "        size.remove(\"\")\n",
    "        if len(size)>= min_length:\n",
    "            filtered_seqs.append(val)\n",
    "    return filtered_seqs\n",
    "\n",
    "def train_test_split(data, train_ratio=0.25):\n",
    "    '''shuffle and split data into train test split'''\n",
    "    random.shuffle(data)\n",
    "    train_range= int(len(data)*train_ratio)\n",
    "    return data[:train_range], data[train_range:]\n",
    "\n",
    "def merge_seq_arr(arr):\n",
    "    '''Merge array of items to space seperated sequence'''\n",
    "    seq=\"\"\n",
    "    for item in arr: \n",
    "         seq+=item + \" \"\n",
    "    return seq\n",
    "    \n",
    "def split_fixed_length_sequence(data, max_length=8):\n",
    "    '''split sequences on fixed lengths to increse data points'''\n",
    "    for seq in data: \n",
    "        if len(seq)> max_length:\n",
    "            pass\n",
    "\n",
    "        \n",
    "def chunks(lst, n=5):\n",
    "    \"\"\"return successive n-sized chunks from lst.\"\"\"\n",
    "    slices=[]\n",
    "#     lst = lst.split(\" \")\n",
    "    for i in range(n, len(lst)+n, n):\n",
    "        slices.append(merge_seq_arr(lst[i-n:i]))\n",
    "\n",
    "    return slices\n",
    "\n",
    "def extend_data_points(data):\n",
    "    \"\"\"Breakes bigger sequences into chunks for length defined in chunks function\"\"\"\n",
    "    extended_data=[]\n",
    "    for seqs in data:\n",
    "        extended_data+=chunks(seqs)\n",
    "    return extended_data\n",
    "\n",
    "def add_padding_left(data, length):\n",
    "    padded_data=[]\n",
    "    for seq in data:\n",
    "        padded_data.append(pad_list(seq.split(\" \"),length))\n",
    "    return padded_data\n",
    "    \n",
    "def pad_list(s, n):\n",
    "    s = [string for string in s if string != \"\"]\n",
    "#     print(\"inside pad function\")\n",
    "#     pdb.set_trace()\n",
    "    return ['PAD'] * (n - len(s)) + s     \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "class W2Vmodel:\n",
    "    PADTOK = 'PAD'\n",
    "    def __init__(self, train_data, vector_size=10, window=1, sg=1, min_count=1, pad=True):\n",
    "        self.pad = pad\n",
    "        self.top_first_item = list(train_data['items'].map(lambda l:l[0]).value_counts().index)[:100]\n",
    "        self.model = Word2Vec(sentences=[self.preprocess(seq) for seq in train_data['items']], min_count=min_count, vector_size=vector_size, window=window, sg=sg)\n",
    "        logging.getLogger(\"gensim\").setLevel(logging.WARNING)\n",
    "        clear_output()\n",
    "        \n",
    "    def preprocess(self, orig_seq):\n",
    "        s = [a.lower() for a in orig_seq]\n",
    "        if self.pad:\n",
    "            return [self.PADTOK] + s\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    def embed(self, seq):\n",
    "        return self.model.wv[seq]\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        # Default is top_first_item if seq is empty\n",
    "        prediction = [(v, 0) for v in self.top_first_item]\n",
    "        if len(seq) > 0:\n",
    "            prediction = self.model.predict_output_word(self.preprocess(seq), topn=10)\n",
    "        return prediction\n",
    "    \n",
    "    def predict_nearest(self, seq):\n",
    "        prediction = [(v, 0) for v in self.top_first_item]\n",
    "        if len(seq) > 0:\n",
    "            emb = self.toks_to_embedding(seq)\n",
    "            prediction = self.nearest_token(emb)\n",
    "        return prediction\n",
    "    \n",
    "    def toks_to_embedding(self, toks):\n",
    "        # TODO: Consider ignoring PAD here\n",
    "        token_embs = [self.token_embedding(t) for t in self.preprocess(toks)]\n",
    "        return np.mean(token_embs, axis=0)\n",
    "\n",
    "    def nearest_token(self, query_emb):\n",
    "        return self.model.wv.most_similar(query_emb)\n",
    "\n",
    "    def token_embedding(self, token_id):\n",
    "        return self.model.wv[token_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22279\n"
     ]
    }
   ],
   "source": [
    "included_data = seqs[seqs['length']>0]\n",
    "\n",
    "word2vec_model= W2Vmodel(included_data, vector_size=20, window=1, sg=1, min_count=1, pad=True)\n",
    "\n",
    "filter_data= seqs[seqs['length']>2]['items']\n",
    "filter_data= extend_data_points(filter_data)\n",
    "filter_data= filter_sequence(filter_data, min_length=5)\n",
    "padded_filter_data= add_padding_left(filter_data,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135597, 4, 20) (135597, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "X, Y=[],[]\n",
    "X_org,Y_org=[],[]\n",
    "\n",
    "\n",
    "for seq in padded_filter_data:\n",
    "    x,y= seq[0:4],seq[4:5]\n",
    "    X_org.append(x)\n",
    "    Y_org.append(y)\n",
    "    X.append(word2vec_model.embed(x))\n",
    "    Y.append(word2vec_model.embed(y))\n",
    "\n",
    "X,Y=np.array(X), np.array(Y)\n",
    "X_org, Y_org=np.array(X_org), np.array(Y_org)\n",
    "\n",
    "\n",
    "print(X.shape,Y.shape)\n",
    "np.save('data/X.data', X)\n",
    "np.save('data/y.data', Y)\n",
    "np.save('data/X_org.data', X_org)\n",
    "np.save('data/y_org.data', Y_org)\n",
    "word2vec_model.model.save(\"data/word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
