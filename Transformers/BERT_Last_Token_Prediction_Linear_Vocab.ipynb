{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943ce4b9",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from transformers import AdamW\n",
    "from transformers import RobertaForMaskedLM\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37042b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_data(path):\n",
    "    '''read sequences from '''\n",
    "    with open(path, 'r') as f:\n",
    "        seqs= f.read().split('\\n')\n",
    "    return seqs\n",
    "\n",
    "def filter_sequence(seqs, min_length=3):\n",
    "    '''filter a sequnce on minimum length'''\n",
    "    filtered_seqs=[]\n",
    "        \n",
    "    for val in seqs:\n",
    "        temp= [value for value in val.split(\" \") if value != '']\n",
    "        if len(temp)>= min_length:\n",
    "            filtered_seqs.append(val)\n",
    "    return filtered_seqs\n",
    "\n",
    "\n",
    "def train_test_split(data, train_ratio=0.25):\n",
    "    '''shuffle and split data into train test split'''\n",
    "    random.shuffle(data)\n",
    "    train_range= int(len(data)*train_ratio)\n",
    "    return data[:train_range], data[train_range:]\n",
    "\n",
    "def merge_seq_arr(arr):\n",
    "    '''Merge array of items to space seperated sequence'''\n",
    "    seq=\"\"\n",
    "    for item in arr: \n",
    "         seq+=item + \" \"\n",
    "    return seq\n",
    "    \n",
    "def split_fixed_length_sequence(data, max_length=9):\n",
    "    '''split sequences on fixed lengths to increse data points'''\n",
    "    for seq in data: \n",
    "        if len(seq)> max_length:\n",
    "            pass\n",
    "\n",
    "        \n",
    "def chunks(lst, n=8):\n",
    "    \"\"\"return successive n-sized chunks from lst.\"\"\"\n",
    "    slices=[]\n",
    "    lst = lst.split(\" \")\n",
    "    for i in range(n, len(lst)+n, n):\n",
    "        slices.append(merge_seq_arr(lst[i-n:i]))\n",
    "\n",
    "    return slices\n",
    "\n",
    "def extend_data_points(data, chunk_size=9):\n",
    "    extended_data=[]\n",
    "    for seqs in data:\n",
    "        extended_data+=chunks(seqs, chunk_size)\n",
    "    return extended_data\n",
    "\n",
    "#masking function\n",
    "def mlm(tensor):\n",
    "    rand= torch.rand(tensor.shape) #[0,1]\n",
    "    #masking tokens that are not seq, /seq, pad\n",
    "    mask_arr = (rand < 0.30) * (tensor > 2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist() #[[2,3,: non zero indicies]]\n",
    "        tensor[i, selection]= 4 # assigning mask token to the selected token\n",
    "    return tensor \n",
    "\n",
    "\n",
    "def mfm(tensor):\n",
    "    counter=0\n",
    "    for i in range(tensor.shape[0]):\n",
    "        idx=tensor[i].detach().clone().tolist().index(2)\n",
    "        tensor[i][idx-1]=4\n",
    "        counter+=1\n",
    "\n",
    "    return tensor \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c827de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model input data lists\n",
    "input_ids =[]\n",
    "mask =[]\n",
    "labels= []\n",
    "path = \"vocab.txt\"\n",
    "\n",
    "    \n",
    "base_seq= read_data(path)\n",
    "fil_seq= filter_sequence(base_seq)\n",
    "test_seq, train_seq= train_test_split(fil_seq)\n",
    "\n",
    "updated_train= extend_data_points(train_seq)\n",
    "updated_test= extend_data_points(test_seq)\n",
    "\n",
    "updated_train= filter_sequence(updated_train)\n",
    "updated_test= filter_sequence(updated_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9cf53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mapping={\"<s>\":0,\"<pad>\":1,\"</s>\":2,\"<unk>\":3,\"<mask>\":4,}\n",
    "iter_val=5\n",
    "\n",
    "for seqs in base_seq:\n",
    "    tokens=[value for value in seqs.split(\" \") if value != '']\n",
    "    unique_token=list(set(tokens))\n",
    "    \n",
    "    for token in unique_token:\n",
    "        if token not in token_mapping:\n",
    "            token_mapping[token] = iter_val\n",
    "            iter_val+=1\n",
    "\n",
    "import json \n",
    "\n",
    "with open(\"vocab_.json\", \"w\") as outfile:\n",
    "    json.dump(token_mapping, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b884691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer(token_mapping, seqs, max_length=11 ):\n",
    "    \n",
    "    attention_masks=[]\n",
    "    input_ids=[]\n",
    "    \n",
    "    for seq in tqdm(seqs):\n",
    "        encoded_sequence=[]\n",
    "        mask_sequence=[]\n",
    "        \n",
    "        #starting token \n",
    "        encoded_sequence.append(token_mapping['<s>'])\n",
    "        \n",
    "        #breaking sequence in tokens\n",
    "        seq=[value for value in seq.split(\" \")[0:5] if value != '']\n",
    "        \n",
    "        #appending encoded items to sequence\n",
    "        for token in seq:\n",
    "            encoded_sequence.append(token_mapping[token])\n",
    "            \n",
    "        #appending ending token\n",
    "        encoded_sequence.append(token_mapping['</s>'])\n",
    "        \n",
    "        mask_sequence=[1]* len(encoded_sequence)\n",
    "        \n",
    "        #padding to max length\n",
    "        if len(encoded_sequence)< max_length:\n",
    "            for i in range(max_length-len(encoded_sequence)):\n",
    "                encoded_sequence.append(token_mapping['<pad>'])\n",
    "                mask_sequence.append(0)\n",
    "                \n",
    "\n",
    "        attention_masks.append(mask_sequence)\n",
    "        input_ids.append(encoded_sequence)\n",
    "\n",
    "    return torch.tensor(attention_masks), torch.tensor(input_ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "831fbbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 54958/54958 [00:00<00:00, 330691.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#initialize model input data lists\n",
    "input_ids =[]\n",
    "mask =[]\n",
    "labels= []\n",
    "\n",
    "\n",
    "mask, input_ids= tokenizer(token_mapping, updated_train, max_length=11)\n",
    "vocab_size= len(token_mapping.keys())\n",
    "\n",
    "    \n",
    "# sample = tokenizer(updated_train, max_length= 11, padding= 'max_length', truncation=True, return_tensors='pt')\n",
    "labels=(input_ids)\n",
    "# mask=(sample.attention_mask)\n",
    "input_ids=(mfm(input_ids.detach().clone()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e17076e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings ={'input_ids': input_ids, 'attention_mask': mask , 'labels':labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a18d87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f9239d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size= 32, shuffle =True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7262de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=vocab_size,  # we align this to the tokenizer vocab_size\n",
    "    max_position_embeddings=11,\n",
    "    hidden_size=282,\n",
    "    num_attention_heads=6,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41082bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8525220\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = RobertaForMaskedLM(config)\n",
    "print(model.num_parameters())\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "print (device)\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f14c0b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b14b61f697040d9a697e6c7403c3b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e7afde89ee4435ab62a7790fb72b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1892cd93b6744e68a1e99fb850707b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f314a6ad97445b8188bef2544f4fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cb052d83714c9bbd385c84bdc1fe42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05751a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('eth_items')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6a38e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size= 64, shuffle =True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e678a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "a={1:'a',2:'b'}\n",
    "\n",
    "inv_map = {v: k for k, v in a.items()}\n",
    "\n",
    "print(inv_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d16aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Custom Inference Code'''\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "\n",
    "\n",
    "def decode_sequence(token_mapping, seqs):\n",
    "    \n",
    "    out=[]\n",
    "    reverse_map= {v: k for k, v in token_mapping.items()}\n",
    "    \n",
    "    for token in seqs:\n",
    "        out.append(reverse_map[token])\n",
    "\n",
    "    print(\"encoding\",seqs, out)\n",
    "    return out\n",
    "    \n",
    "\n",
    "def decode_inference(output,labels, token_mapping, topN=10):\n",
    "    decoded_out=[]\n",
    "    \n",
    "\n",
    "\n",
    "    for i in tqdm(range(len(output))):\n",
    "\n",
    "        mask_list=labels[i].tolist()\n",
    "        \n",
    "        idx= mask_list.index(2) - 1 # label for mask\n",
    "        \n",
    "        import pdb; pdb.set_trace()\n",
    "        pred= np.array(output[i][idx].detach().tolist())\n",
    "        pred=heapq.nlargest(topN, range(len(pred)), pred.take)\n",
    "\n",
    "        out_decoded= decode_sequence(token_mapping, pred)\n",
    "            \n",
    "\n",
    "        decoded_out.append(out_decoded)\n",
    "    return decoded_out\n",
    "\n",
    "\n",
    "def compute_acc(output,labels, token_mapping, topN=10):\n",
    "    decoded_out=[]\n",
    "    \n",
    "    correct=0\n",
    "\n",
    "    for i in tqdm(range(len(output))):\n",
    "\n",
    "        mask_list=labels[i].tolist()\n",
    "        \n",
    "        idx= mask_list.index(2) - 1 # label for mask\n",
    "        \n",
    "#         import pdb; pdb.set_trace()\n",
    "        pred= np.array(output[i][idx].detach().tolist())\n",
    "        pred=heapq.nlargest(topN, range(len(pred)), pred.take)\n",
    "        \n",
    "        if labels[i][idx] in pred:\n",
    "#             print(\"Correct\")\n",
    "            correct+=1\n",
    "            \n",
    "\n",
    "\n",
    "    return (correct/len(labels))*100\n",
    "\n",
    "def infer(model, token_mapping ,seqs, topN=9):\n",
    "    \n",
    "\n",
    "    mask, input_ids= tokenizer(token_mapping, seqs, max_length=11)\n",
    "    vocab_size= len(token_mapping.keys())\n",
    "\n",
    "\n",
    "    # sample = tokenizer(updated_train, max_length= 11, padding= 'max_length', truncation=True, return_tensors='pt')\n",
    "    labels=(input_ids)\n",
    "    # mask=(sample.attention_mask)\n",
    "    input_ids=(mfm(input_ids.detach().clone()))\n",
    "\n",
    "    encodings ={'input_ids': input_ids, 'attention_mask': mask , 'labels':labels}\n",
    "    \n",
    "    dataset = Dataset(encodings)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size= 64, shuffle =False)\n",
    "    loader = tqdm(loader, leave=True)\n",
    "    \n",
    "    batch_out=[]\n",
    "    batch_label=[]\n",
    "    \n",
    "    \n",
    "    for batch in loader:\n",
    "\n",
    "        input_ids_ = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        inference= model(input_ids_.cuda(), attention_mask=attention_mask.cuda())\n",
    "        \n",
    "        batch_out+=(inference['logits'].tolist())\n",
    "        batch_label+=(batch['labels'].tolist())\n",
    "        \n",
    "        \n",
    "    print(inference['logits'].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output= compute_acc(torch.tensor(batch_out), torch.tensor(batch_label)  ,token_mapping, topN)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_accuracy(predicted, actual, tok):\n",
    "    correct=0\n",
    "    \n",
    "    print(predicted[0], actual[0])\n",
    "    print(len(predicted), len(actual))\n",
    "    for i in range(len(predicted)):\n",
    "        last_item= [value for value in actual[i].split(\" \") if value != ''][-1]\n",
    "#         predicted_items= [value for value in predicted[i].split(\" \") if value != '']\n",
    "        predicted_items=predicted[i]\n",
    "        \n",
    "        import pdb; pdb.set_trace()\n",
    "        print(\"last Item\", last_item, predicted_items)\n",
    "        if last_item in predicted_items:\n",
    "            \n",
    "            correct+=1\n",
    "            print(\"true\")\n",
    "            \n",
    "    print(correct)\n",
    "    return (correct/len(actual)) *100\n",
    "            \n",
    "        \n",
    "    \n",
    "# print(infer['logits'][0][0])\n",
    "# test_list=infer['logits'][0][10].tolist()\n",
    "\n",
    "\n",
    "# test_list = np.array(test_list)\n",
    "# idx=heapq.nlargest(10, range(len(test_list)), test_list.take)\n",
    "\n",
    "# print(idx)\n",
    "# tokenizer.decode(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6f46683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a79cf54c640eb9c024fa64ada852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb4686415a94fec96462c9b050af12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 11, 762])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519a4d750b5546c5badd855344aeff8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "37.270279122238804\n"
     ]
    }
   ],
   "source": [
    "top_n=10\n",
    "\n",
    "train_results= infer(model, token_mapping, updated_train, top_n)\n",
    "print(\"##################################\")\n",
    "print(train_results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_results= infer(model, token_mapping, updated_test, top_n)\n",
    "# print(\"computing accuracy\")\n",
    "# test_acc= compute_accuracy(test_results, updated_test)\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f03a7d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5870533c575947cf84bb12f221cba215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f1a7f16ca44043859de31ec822b7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 11, 762])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d96008a4d654da2a16850865c9ee403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "27.75425029153624\n"
     ]
    }
   ],
   "source": [
    "test_results= infer(model, token_mapping, updated_test, top_n)\n",
    "print(\"##################################\")\n",
    "print(test_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef6d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
